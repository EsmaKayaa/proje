from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from dotenv import load_dotenv
import os
import json

# .env dosyasÄ±nÄ± yÃ¼kle
load_dotenv()

# .env'den anahtarÄ± gÃ¼venli ÅŸekilde al
GEMINI_API_KEY = os.getenv("GOOGLE_GENAI_API_KEY")

# EÄŸer lokal embedding kullanÄ±yorsanÄ±z True yapÄ±n, API ile kullanÄ±yorsanÄ±z False
USE_LOCAL_EMBEDDING = False 

def get_embeddings_model():
    """KullanÄ±lacak embedding modelini dÃ¶ndÃ¼rÃ¼r."""
    if USE_LOCAL_EMBEDDING:
        # Lokal modeller iÃ§in gerekli kÃ¼tÃ¼phaneyi iÃ§e aktar
        from langchain_community.embeddings import HuggingFaceEmbeddings
        print("ğŸ’¡ Lokal Embedding Modeli KullanÄ±lÄ±yor...")
        return HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    else:
        # Gemini API Embeddings
        if not GEMINI_API_KEY:
            raise ValueError("Google Gemini API anahtarÄ± bulunamadÄ±. LÃ¼tfen .env dosyanÄ±zÄ± kontrol edin.")
        print("ğŸ’¡ Google Generative AI Embeddings Modeli KullanÄ±lÄ±yor...")
        return GoogleGenerativeAIEmbeddings(
            # GÃ¼ncel ve Ã¶nerilen embedding modelini kullan
            model="text-embedding-004", 
            google_api_key=GEMINI_API_KEY 
        )

def build_vector_database(data_path="data/wikipedia_cybersecurity.txt", persist_dir="vector_db"):
    print(f"ğŸ“– {data_path} verisi yÃ¼kleniyor...")
    loader = TextLoader(data_path, encoding="utf-8")
    documents = loader.load()

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=150)
    docs = text_splitter.split_documents(documents)
    print(f"ğŸ“„ Veri {len(docs)} parÃ§aya ayrÄ±ldÄ±.")

    meta_path = "data/wikipedia_meta.json"
    meta = {}
    if os.path.exists(meta_path):
        with open(meta_path, "r", encoding="utf-8") as f:
            meta = json.load(f)

    # Metadata'yÄ± belgelere ekle
    for d in docs:
        d.metadata = d.metadata or {}
        d.metadata["source_title"] = meta.get("title", "Cybersecurity - Wikipedia")
        d.metadata["source_url"] = meta.get("url", "")

    # Embedding modelini al
    embeddings = get_embeddings_model()
    
    # VektÃ¶r DB'yi oluÅŸtur
    os.makedirs(persist_dir, exist_ok=True)
    db = Chroma.from_documents(docs, embedding=embeddings, persist_directory=persist_dir) 
    db.persist()
    print("âœ… VektÃ¶r veritabanÄ± oluÅŸturuldu:", persist_dir)

if __name__ == "__main__":
    if not os.path.exists("data/wikipedia_cybersecurity.txt"):
        print("Veri dosyasÄ± bulunamadÄ±. LÃ¼tfen Ã¶nce data_preparation.py dosyasÄ±nÄ± Ã§alÄ±ÅŸtÄ±rÄ±n.")
    else:
        build_vector_database()
