import streamlit as st
from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from dotenv import load_dotenv 
import os

load_dotenv() # .env dosyasÄ±nÄ± yÃ¼kle

# .env'den anahtarÄ± gÃ¼venli ÅŸekilde al
GEMINI_API_KEY = os.getenv("GOOGLE_GENAI_API_KEY")

# build_vector_db.py dosyanÄ±zdaki ayarla uyumlu olmalÄ±dÄ±r!
USE_LOCAL_EMBEDDING = False 

def get_embeddings_model(api_key):
    """KullanÄ±lacak embedding modelini dÃ¶ndÃ¼rÃ¼r."""
    if USE_LOCAL_EMBEDDING:
        from langchain_community.embeddings import HuggingFaceEmbeddings
        return HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    else:
        # GÃ¼ncel ve Ã¶nerilen embedding modelini kullan
        return GoogleGenerativeAIEmbeddings(
            model="text-embedding-004", 
            google_api_key=api_key
        )

st.set_page_config(page_title="Cybersecurity RAG Chatbot", layout="centered")
st.title("ğŸ¤– Cybersecurity RAG Chatbot")
st.write("Wikipedia'dan alÄ±nmÄ±ÅŸ 'Cybersecurity' iÃ§eriÄŸi ile desteklenen soru-cevap sistemi.")

if not GEMINI_API_KEY:
    st.error("Google Gemini API anahtarÄ± `.env` dosyasÄ±ndan yÃ¼klenemedi. LÃ¼tfen kontrol edin (DeÄŸiÅŸken adÄ±: GOOGLE_GENAI_API_KEY).")
elif not os.path.exists("vector_db"):
    st.warning("VektÃ¶r veritabanÄ± bulunamadÄ±. LÃ¼tfen Ã¶nce `data_preparation.py` ve ardÄ±ndan `build_vector_db.py` dosyalarÄ±nÄ± Ã§alÄ±ÅŸtÄ±rÄ±n.")
else:
    try:
        # 1. Embedding modelini oluÅŸtur
        embeddings = get_embeddings_model(GEMINI_API_KEY)

        # 2. Chroma yÃ¼klenirken embedding fonksiyonu belirtilmeli!
        db = Chroma(
            persist_directory="vector_db",
            embedding_function=embeddings 
        )
        retriever = db.as_retriever(search_kwargs={"k": 3})

        # LLM modelini oluÅŸtur (Daha yetenekli Gemini modelini kullandÄ±k)
        llm = ChatGoogleGenerativeAI(
            model="gemini-2.5-flash", # GÃ¼ncel LLM modeli
            google_api_key=GEMINI_API_KEY 
        )

        qa = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=retriever,
            return_source_documents=True
        )

        # Streamlit ArayÃ¼zÃ¼
        question = st.text_input("Sorunu yaz ve Enter'a bas veya 'GÃ¶nder' butonuna tÄ±kla:")
        if st.button("GÃ¶nder") and question:
            with st.spinner("Cevap aranÄ±yor..."):
                output = qa({"query": question})
                answer = output["result"]
                sources = output.get("source_documents", [])

            st.markdown("### âœ… Cevap")
            st.write(answer)

            st.markdown("### ğŸ“š KullanÄ±lan Kaynaklar")
            for i, d in enumerate(sources):
                snippet = d.page_content[:400].replace("\n", " ") + ("..." if len(d.page_content) > 400 else "")
                url = d.metadata.get("source_url", "")
                title = d.metadata.get("source_title", "Wikipedia")
                st.write(f"**[{i+1}] {title}** | {url}")
                st.code(snippet, language='text')
    except Exception as e:
        st.error(f"Uygulama baÅŸlatÄ±lÄ±rken bir hata oluÅŸtu: {e}")
        st.error("LÃ¼tfen API anahtarÄ±nÄ±zÄ±n geÃ§erli olduÄŸundan ve tÃ¼m baÄŸÄ±mlÄ±lÄ±klarÄ±n yÃ¼klÃ¼ olduÄŸundan emin olun.")
